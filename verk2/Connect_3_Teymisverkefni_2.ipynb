{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/Culmenus/Teymi5-TV2/blob/main/Connect_3_Teymisverkefni_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Teymisverkefni 2**\n",
    "\n",
    "Í teymisverkefni-2 er markmiðið að læra leikáætlun með því að spila á móti annarri leikáætlun sem er líka að læra (e. both sides are learning). Sjá umræðu í Sutton and Barto (fyrsta kafla) um self-play og tic-tac-toe. \n",
    "\n",
    "Hvert teymi skilar **teymiX.npy** skrá með stefnu $\\pi(s,a)$ þar sem $s$ er skilgreint  með Zobrist hashing (sjá neðar) ásamt læsilegri og auðskiljanlegri útfærslu á reikniriti.\n",
    "\n",
    "Frjálst er að velja \"reinforcement learning\" reiknirit sem við höfum fjallað um í þessari námslotu. Sem dæmi, það má vera TD($0$), MC, TD($\\lambda$), $Q-$learning, SARSA($\\lambda$), expected SARSA($\\lambda$). Svo þarf að spá í hvernig *exploration* er útfært.\n",
    "\n",
    "Áður en þið byrjið skulum við ræða eftirfarandi æfingar úr Sutton og Barto:\n",
    "\n",
    "  1. (Exercise 1.1): **Self-Play** Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n",
    "\n",
    "  2. (Exercise 1.2): **Symmetries** Some Connect-3 positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n",
    "\n",
    "  3. Exercise 1.3: **Greedy Play** Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?\n",
    "\n",
    "  4. Exercise 1.4: **Learning from Exploration** Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n",
    "\n",
    "  5. *Exercise 1.5*: **Other Improvements** Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the Connect-3 problem as posed?"
   ],
   "metadata": {
    "id": "dqIXTrr03QfY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {
    "id": "o6KO7AsvJezQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Connect-3 a mini version of Connect-4 on a 5x5 board**\n",
    "\n",
    "The following code implements the game Connect-3 on a 5-by-5 board. We will walk through this code in class. I am open for any improvement you may suggest for making the code faster. The Zobrist hashing table is randomly generated using the seed 42. Don't change how we generate the hashed states, I aim to let your policy compete against policies generated by other teams. This final exercise, will be used as input to our discussion on Sutton and Barto's exercise 1.1."
   ],
   "metadata": {
    "id": "WMGDAjrzJOVt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Sindri Testing\r\n",
    "# two player game (1) versus (2)\r\n",
    "getotherplayer = lambda p : 3-p # returns the other player\r\n",
    "# the initial empty board, in matrix board we store legal indices in board[:,-1]\r\n",
    "def iState(n = 5, m = 5):\r\n",
    "  return np.zeros((n,m+1), dtype=np.uint16)\r\n",
    "# perform move for player on board\r\n",
    "def Action(board, move, player):\r\n",
    "  if board[move,-2] > 0:\r\n",
    "    print(\"illegal move \", move, \" for board \", np.flipud(board.T))\r\n",
    "    raise\r\n",
    "  else:\r\n",
    "    board[move,board[move,-1]] = player # place the disc on board\r\n",
    "    board[move,-1] += 1 # next legal drop\r\n",
    "  return board\r\n",
    "# determine if terminal board state, assuming last move was made by player\r\n",
    "def Terminal(board, player, n = 5, m = 5):\r\n",
    "  # now we need to see if we can find any 3 in a row\r\n",
    "  # here are all the possible ways of doing so on a 3x3 matrix\r\n",
    "  swin = np.array(\r\n",
    "          [[True,  True,  True,  False, False, False, False, False, False], # these are all possible winning states\r\n",
    "           [False, False, False, True,  True,  True,  False, False, False], # for three in a row\r\n",
    "           [False, False, False, False, False, False, True,  True,  True],\r\n",
    "           [True,  False, False, False, True,  False, False, False, True],\r\n",
    "           [False, False, True,  False, True,  False, True,  False, False],\r\n",
    "           [True,  False, False, True,  False, False, True,  False, False],\r\n",
    "           [False, True,  False, False, True,  False, False, True,  False],\r\n",
    "           [False, False, True,  False, False, True,  False, False, True]]\r\n",
    "          )\r\n",
    "  for i in range(n-2):\r\n",
    "    for j in range(m-2): # scan all 3x3 over the board\r\n",
    "      b3x3 = np.ones((8,1)) @ board[i:(i+3),j:(j+3)].reshape(1,9) # extract 3x3 segment \r\n",
    "      if np.any(np.sum((b3x3 == player) & swin, axis = 1) == 3): # check if 3 in a row\r\n",
    "        return True\r\n",
    "  return False\r\n",
    "# Some pretty way of displaying the board in the terminal\r\n",
    "def pretty_print(board, n = 5, m = 5, symbols = \" XO\"):\r\n",
    "  for num in range(1, n+1):\r\n",
    "    print(\" \" + str(num) + \" \", end = \" \")\r\n",
    "  print()\r\n",
    "  for j in range(m):\r\n",
    "    for i in range(n):\r\n",
    "      print(\" \" + symbols[board[i,m-1-j]] + \" \", end = \" \")\r\n",
    "    print(\"\")\r\n",
    "# let's simulate a single game using pure random play, i.e. demonstrate an episode!\r\n",
    "def connect3():\r\n",
    "  S = iState() # initial board state\r\n",
    "  p = 1 # first player to move (other player is 2)\r\n",
    "  a = np.random.choice(np.where(S[:,-2]==0)[0],1) # first move is random\r\n",
    "  S = Action(S,int(a),p) # force first move to be random\r\n",
    "  p = getotherplayer(p) # other player's turn \r\n",
    "  while True:\r\n",
    "    a = np.random.choice(np.where(S[:,-2]==0)[0],1) # pure random policy\r\n",
    "    if 0 == len(a): # check if a legal move was possible, else bail out\r\n",
    "      return 0, S # its a draw, return 0 and board\r\n",
    "    S = Action(S,int(a),p) # take action a and update the board state\r\n",
    "    if Terminal(S,p):\r\n",
    "      return p, S # return the winning player and board\r\n",
    "    p = getotherplayer(p) # other player's turn\r\n",
    "  return 0, S # default is a draw\r\n",
    "\r\n",
    "# run demo for random play policy:\r\n",
    "winner, board = connect3()\r\n",
    "symbols = \" XO\"\r\n",
    "print(\" winner is '\", symbols[winner],\"' final board is:\\n\")\r\n",
    "pretty_print(board)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " winner is ' O ' final board is:\n",
      "\n",
      " 1   2   3   4   5  \n",
      "     X       X      \n",
      "     O   O   X   O  \n",
      "     X   X   O   O  \n",
      " X   O   O   X   X  \n",
      " O   X   X   O   O  \n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMh2R_LSJNtb",
    "outputId": "752cdcc2-0d7c-4681-86c9-980d55249f3f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Constructing value and action value tables using [Zobrist hashing](https://en.wikipedia.org/wiki/Zobrist_hashing):**\n",
    "\n",
    "Please do not modify how the zobTable is generated.\n"
   ],
   "metadata": {
    "id": "FktNHPhJZ5mL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Sindri Testing 2\r\n",
    "# let's all use the same zobTable, so we set the random seed\r\n",
    "np.random.seed(42)\r\n",
    "zobTable = np.random.randint(1,2**(5*5)-1, size=(5,5,3), dtype = np.uint32)\r\n",
    "# compute index from current board state\r\n",
    "def computeHash(board, n = 5, m = 5):\r\n",
    "  h = 0\r\n",
    "  for i in range(n):\r\n",
    "    for j in range(board[i,-1]):\r\n",
    "      h ^= zobTable[i,j,board[i,j]]\r\n",
    "  return h"
   ],
   "outputs": [],
   "metadata": {
    "id": "adWrLjydTPp1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Sindri Testing 3\r\n",
    "# playing around with these hashed values\r\n",
    "maxhashValue = 2**(5*5)\r\n",
    "# Initialize Q(s,a) function\r\n",
    "Q = np.zeros((maxhashValue,5))\r\n",
    "# Access a particular (Q(s,a))\r\n",
    "a = 0 # any legal move in {0,1,2,3,4}\r\n",
    "player = 1 # now you need to think which player owns these Q values...\r\n",
    "Q[computeHash(board),a]\r\n",
    "# An afterstate value function (more efficient implementation)\r\n",
    "V = np.zeros(maxhashValue)\r\n",
    "# Access a particular (V(s_a)), here you need to do a one-step lookahead (afterstate)\r\n",
    "hash_key = computeHash(board)\r\n",
    "lookaheadBoard = board.copy()\r\n",
    "lookaheadBoard = Action(lookaheadBoard,a,player)\r\n",
    "lookahead_hash_key = computeHash(lookaheadBoard)\r\n",
    "print(\"(from lookaheadBoard lookahead_hash_key = \", lookahead_hash_key, \"hash_key = \", hash_key)\r\n",
    "V[lookahead_hash_key]\r\n",
    "# Now check this out, the novelty of Zobrist hashing\r\n",
    "(i,j) = (a,board[a,-1]) # where we would like to add our player\r\n",
    "lookahead_hash_key = hash_key\r\n",
    "lookahead_hash_key ^= zobTable[i,j,player] # create move without updating the board!\r\n",
    "print(\"lookahead_hash_key = \", lookahead_hash_key, \"hash_key = \", hash_key)\r\n",
    "lookahead_hash_key ^= zobTable[i,j,player] # undo the move without updating the board!\r\n",
    "print(\"(undo)lookahead_hash_key = \", lookahead_hash_key, \"hash_key = \", hash_key)\r\n",
    "# Save your policy, PI to file, see folder icon on left hand side to download!\r\n",
    "np.save(\"teymi5\", V)\r\n",
    "!ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(from lookaheadBoard lookahead_hash_key =  18267665 hash_key =  31298052\n",
      "lookahead_hash_key =  18267665 hash_key =  31298052\n",
      "(undo)lookahead_hash_key =  31298052 hash_key =  31298052\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPVA4yFLvwvU",
    "outputId": "0788289d-c760-4e85-e4c5-c4a16bc5ee3c"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Connect-3: Teymisverkefni 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "5e1f006ae03b804e29419bc04856ea9a7b4c4376a396d891cefd79eb15def696"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}